{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b1b03e",
   "metadata": {
    "id": "f7b1b03e"
   },
   "source": [
    "# Credit Risk Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf80a2d",
   "metadata": {
    "id": "cdf80a2d"
   },
   "source": [
    "This project involves an in-depth analysis of a loan dataset from a consumer finance company. It aim to identify patterns and factors that indicate whether a loan applicant is likely to default on the loan, enabling the lending company to make informed lending decisions and minimize credit loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7c2de",
   "metadata": {
    "id": "efe7c2de"
   },
   "source": [
    "## 1. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ANNzunYWIsjh",
   "metadata": {
    "id": "ANNzunYWIsjh"
   },
   "source": [
    "### a. Reading Dataset and understanding general structure of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09b2b5",
   "metadata": {
    "id": "ab09b2b5"
   },
   "outputs": [],
   "source": [
    "#Importing pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d193428",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d193428",
    "outputId": "d02bb9e8-20d3-4f07-aa75-7e9f6daf8139"
   },
   "outputs": [],
   "source": [
    "# Reading the dataset into a DataFrame\n",
    "original_df = pd.read_csv(\"loan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff4447",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "9cff4447",
    "outputId": "f3cbe7bc-9f44-49ef-dcf9-f6f18c61098e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Displaying first 5 rows of the dataframe\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6890ff4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6890ff4",
    "outputId": "7a94a69a-dc19-46bc-ab08-530ae6829df7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe38d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cbe38d3",
    "outputId": "35505301-b855-489d-ce12-2537a5341af6"
   },
   "outputs": [],
   "source": [
    "original_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e46dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7e46dcd",
    "outputId": "ce2543ca-daaa-47df-9f06-869afb6b3645"
   },
   "outputs": [],
   "source": [
    "print(f\"Total number of columns: {original_df.shape[0]}\")\n",
    "print(f\"Total number of rows: {original_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y8Xy30lDN-P9",
   "metadata": {
    "id": "y8Xy30lDN-P9"
   },
   "source": [
    "### b. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566fd90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "7566fd90",
    "outputId": "6ecbd5a6-a391-46e8-f588-e1108c36790a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1iwS7Nzjk2Fz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iwS7Nzjk2Fz",
    "outputId": "ee440079-2b47-4c01-c2ed-5fa651575ecd"
   },
   "outputs": [],
   "source": [
    "original_df['loan_amnt'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cMBgxYW8lahi",
   "metadata": {
    "id": "cMBgxYW8lahi"
   },
   "outputs": [],
   "source": [
    "original_df['loan_amnt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U4XltFCblm5K",
   "metadata": {
    "id": "U4XltFCblm5K"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count unique categories\n",
    "unique_categories = original_df['loan_amnt'].nunique()\n",
    "\n",
    "# Calculate category counts\n",
    "category_counts = original_df['loan_amnt'].value_counts()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Categorical Variable')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RnanrczWi9ii",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RnanrczWi9ii",
    "outputId": "fb4d3c83-a3d5-4d58-af50-20e3c32f9f95"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select few numerical columns for visualizing the\n",
    "numerical_columns = ['loan_amnt', 'int_rate', 'funded_amnt', 'installment', 'annual_inc'] # Visualizing only few columns\n",
    "\n",
    "# Create histograms for each numerical column\n",
    "for col in numerical_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(original_df[col], bins=20, edgecolor='k')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gHage1cOjwCe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "gHage1cOjwCe",
    "outputId": "b243c622-6f73-4fe7-bd6a-e8a1f1d16aa6"
   },
   "outputs": [],
   "source": [
    "# Create box plots for each numerical column\n",
    "for col in numerical_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=original_df[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KWkZTQi8jvpz",
   "metadata": {
    "id": "KWkZTQi8jvpz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "_ZQVdHkzOBzj",
   "metadata": {
    "id": "_ZQVdHkzOBzj"
   },
   "source": [
    "Different statistics can be observed like (mean, median, min, max, etc)<br>\n",
    "For few columns like loan_amnt, funded_amnt, it is quite understandable.<br>\n",
    "But for few columns like num_tl_op_past_12m, pct_tl_nvr_dlq, etc it is non understandable due to irrelavant data. Perhaps that can be cleared in data cleaning process by different strategies like dropping null, outliers etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l_7LDA_dI_6R",
   "metadata": {
    "id": "l_7LDA_dI_6R"
   },
   "source": [
    "### d. Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a4285",
   "metadata": {
    "id": "a93a4285"
   },
   "source": [
    "Since we can see lots of null values in our previous observations, let's check the occurance of null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb76e8",
   "metadata": {
    "id": "2bbb76e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculating the number of columns with all null values\n",
    "num_columns_with_all_null = original_df.isnull().all().sum()\n",
    "print(f\"Number of columns with all null values: {num_columns_with_all_null}\")\n",
    "\n",
    "# Calculating the number of columns with at least one null values\n",
    "num_columns_with_missing_values = original_df.columns[original_df.isnull().any()].nunique()\n",
    "print(f\"Number of columns with at least one null values: {num_columns_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bJ7jT1CtU-ol",
   "metadata": {
    "id": "bJ7jT1CtU-ol"
   },
   "source": [
    "Now since we have checked all the null values in the dataset. Let's see the columns where all the values are either *zero/same* i.e. `consistent values in the columns`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TxJN4WezWXiG",
   "metadata": {
    "id": "TxJN4WezWXiG"
   },
   "source": [
    "### e. Consistent Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WeUK_JIvWbkt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeUK_JIvWbkt",
    "outputId": "942233e5-f727-476c-aed4-45dfaea805c3"
   },
   "outputs": [],
   "source": [
    "# Checking the columns having all the values same\n",
    "same_value_columns = original_df.columns[original_df.nunique() == 1]\n",
    "\n",
    "# Print the list of columns with the same values and their values\n",
    "for column in same_value_columns:\n",
    "    value = original_df[column].iloc[0]\n",
    "    print(f\"Column: '{column}' --> {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKuHgm3XXzWW",
   "metadata": {
    "id": "ZKuHgm3XXzWW"
   },
   "source": [
    "These columns can be droped since it doesn't have any use in our analysis because all the values are same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MnfAKeCPJW44",
   "metadata": {
    "id": "MnfAKeCPJW44"
   },
   "source": [
    "### e. Data Format and Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cPB7OTVOL2_p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPB7OTVOL2_p",
    "outputId": "e9be725a-3a57-4f7c-ff29-e29167706a33"
   },
   "outputs": [],
   "source": [
    "data_types = original_df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vqYylZoMZG23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqYylZoMZG23",
    "outputId": "15063ba2-e8b8-47fc-b816-40aa401ff12f"
   },
   "outputs": [],
   "source": [
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1WjxiYleJjHo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WjxiYleJjHo",
    "outputId": "b191feb5-2a02-4417-c1d5-fbf551732358"
   },
   "outputs": [],
   "source": [
    "data_types.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F_pHE87kL6jC",
   "metadata": {
    "id": "F_pHE87kL6jC"
   },
   "source": [
    "We can see here there are three types of data types 'int64', 'float64', 'O'(object)<br>\n",
    "Data types of few columns like term, int_rate, etc need to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2Hrr7m804OYg",
   "metadata": {
    "id": "2Hrr7m804OYg"
   },
   "source": [
    "### f. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_QTk9uJL4NdZ",
   "metadata": {
    "id": "_QTk9uJL4NdZ"
   },
   "outputs": [],
   "source": [
    "# Write a code to calculate outlier for few numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DZmDPbJiexOf",
   "metadata": {
    "id": "DZmDPbJiexOf"
   },
   "source": [
    "### g. Variable meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Fm1OhVMevnA",
   "metadata": {
    "id": "2Fm1OhVMevnA"
   },
   "outputs": [],
   "source": [
    "# Print the names of all variables\n",
    "original_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SJmVW5nNgIlZ",
   "metadata": {
    "id": "SJmVW5nNgIlZ"
   },
   "source": [
    "Here we can see lots of variables name doesn't reflect to the actual meaning and is difficult to understand. Every time we have to refer Data Dictionary. So, name of the columns should also be changed in further cleaning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wa-mlH6iw-4H",
   "metadata": {
    "id": "Wa-mlH6iw-4H"
   },
   "source": [
    "### h. Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WEGuEqBOnupM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEGuEqBOnupM",
    "outputId": "a37f9887-4e22-438b-8662-71def94da77e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'original_df'\n",
    "\n",
    "# Select only numerical columns\n",
    "numerical_columns = original_df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate skewness for each numerical column\n",
    "skewness = numerical_columns.apply(lambda x: x.skew())\n",
    "\n",
    "# Display skewness values\n",
    "print(\"Skewness of numerical columns:\")\n",
    "print(skewness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dICawNe-oSSj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dICawNe-oSSj",
    "outputId": "501fab80-2e78-4342-bdf6-423314a8adf5"
   },
   "outputs": [],
   "source": [
    "# Assuming 'skewness' is a Series containing skewness values\n",
    "normally_distributed = sum((-0.5 <= skewness) & (skewness <= 0.5))\n",
    "left_skewed = sum(skewness < -0.5)\n",
    "right_skewed = sum(skewness > 0.5)\n",
    "\n",
    "print(\"Normally Distributed Variables:\", normally_distributed)\n",
    "print(\"Left-Skewed Variables:\", left_skewed)\n",
    "print(\"Right-Skewed Variables:\", right_skewed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1szCDptr6cF",
   "metadata": {
    "id": "k1szCDptr6cF"
   },
   "source": [
    "Among 111 variables in dataset only skewness of 33 has been calculated. <br>\n",
    "It either means rest other columns are non numerical variables OR the columns have all value missing.<br>\n",
    "Now, it can be seen that dataset are not at all distributed normally. So, it need to be cleaned in various way like, dropping missing values, removing outliers, changing datatypes of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CBCMeHx0NEdZ",
   "metadata": {
    "id": "CBCMeHx0NEdZ"
   },
   "source": [
    "### i. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rg44lmlV6Myb",
   "metadata": {
    "id": "rg44lmlV6Myb"
   },
   "source": [
    "a. **Reading Dataset and understanding general structure of the dataset**:\n",
    "   - We successfully loaded the dataset into `original_df` and viewed the general structure of the dataset, including its shape, number of columns and rows, etc.\n",
    "\n",
    "b. **Summary Statistics**:\n",
    "   - Providing summary statistics (mean, median, max, min, etc.) for numerical data helped us understand the central tendencies and spread of the data, enabling us to consider further steps.\n",
    "\n",
    "c. **Missing Values**:\n",
    "   - For columns with all null values, we decided to drop those columns.\n",
    "   - For columns with at least one null value, we applied a threshold:\n",
    "     - Columns with null values exceeding the threshold were dropped.\n",
    "     - Columns with null values below the threshold were candidates for imputation or could involve dropping rows with null values.\n",
    "\n",
    "d. **Consistent values in columns**:\n",
    "   - Columns with all values as zero or the same value were dropped.\n",
    "\n",
    "e. **Data Format and Types**:\n",
    "   - We observed issues with data format, such as string values for dates, and noted that they needed to be transformed during the data cleaning process.\n",
    "   - We also identified the need to change the data types of certain variables.\n",
    "\n",
    "f. **Outliers**:\n",
    "   - We detected numerous outliers in the variables, highlighting the importance of addressing them during the data cleaning process.\n",
    "\n",
    "g. **Variable Meaning**:\n",
    "   - Some variable names were found to be unclear or not reflective of their actual meaning, indicating the need for renaming during the cleaning process.\n",
    "\n",
    "h. **Data Distribution**:\n",
    "   - Observing the data distribution, we noted that most variables were not normally distributed. This prompted us to consider various data cleaning approaches, including dropping missing values, removing outliers, changing variable data types, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tgNe-8157T3z",
   "metadata": {
    "id": "tgNe-8157T3z"
   },
   "source": [
    "Gramatically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ULJgyT7NWV",
   "metadata": {
    "id": "b7ULJgyT7NWV"
   },
   "source": [
    "a. **Reading the Dataset and Understanding the General Structure**:\n",
    "   - We successfully loaded the dataset into `original_df` and examined its general structure, including its shape, the number of columns, and rows.\n",
    "\n",
    "b. **Summary Statistics**:\n",
    "   - Providing summary statistics (mean, median, max, min, etc.) for numerical data helped us gain insights into central tendencies and data dispersion, facilitating further steps.\n",
    "\n",
    "c. **Handling Missing Values**:\n",
    "   - We addressed missing values by considering two scenarios:\n",
    "     - Columns with all null values: We decided to drop these columns.\n",
    "     - Columns with at least one null value: We applied a threshold:\n",
    "       - Columns with null values exceeding the threshold were dropped.\n",
    "       - For columns with null values below the threshold, we considered options such as imputation or removing rows with null values.\n",
    "\n",
    "d. **Removing Columns with Consistent Values**:\n",
    "   - We removed columns where all values were either zero or the same.\n",
    "\n",
    "e. **Data Format and Types**:\n",
    "   - We identified issues related to data format, including string values for dates, which needed transformation during the data cleaning process.\n",
    "   - Additionally, we recognized the need to change the data types of specific variables.\n",
    "\n",
    "f. **Handling Outliers**:\n",
    "   - We detected numerous outliers in the variables, emphasizing the importance of addressing them during the data cleaning process.\n",
    "\n",
    "g. **Clarifying Variable Names**:\n",
    "   - Some variable names were found to be unclear or not reflective of their actual meaning, highlighting the need for renaming during the cleaning process.\n",
    "\n",
    "h. **Data Distribution**:\n",
    "   - Observing the data distribution, we noted that most variables were not normally distributed. This prompted us to consider various data cleaning approaches, including dropping missing values, removing outliers, changing variable data types, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wMREQ6-9dJzL",
   "metadata": {
    "id": "wMREQ6-9dJzL"
   },
   "source": [
    "Before starting a cleaning process lets keep the backup of original data frame to `original_df` and copy the dataframe to `df` and we will be using that for further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443a588",
   "metadata": {
    "id": "b443a588"
   },
   "outputs": [],
   "source": [
    "df = original_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jJETaHE27wnL",
   "metadata": {
    "id": "jJETaHE27wnL"
   },
   "source": [
    "## 2. Data Cleaning and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hmh3ymjQZt_y",
   "metadata": {
    "id": "Hmh3ymjQZt_y"
   },
   "outputs": [],
   "source": [
    "# 68 columns have null values in it. But, deleting all those column might result to the loss of data.\n",
    "# Lets delete the columns with more than 30% of the values null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u3VEAprAXkX6",
   "metadata": {
    "id": "u3VEAprAXkX6"
   },
   "outputs": [],
   "source": [
    "# Define a threshold for the maximum allowable missing values\n",
    "max_missing_threshold = 0.3\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "columns_with_missing_values = df.isnull().mean()\n",
    "\n",
    "# Drop columns with missing values exceeding the threshold\n",
    "columns_to_drop = columns_with_missing_values[columns_with_missing_values > max_missing_threshold].index\n",
    "\n",
    "# Drop the identified columns from the DataFrame\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h3xSC2qKYMwa",
   "metadata": {
    "id": "h3xSC2qKYMwa"
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gi3yu_tzd--i",
   "metadata": {
    "id": "gi3yu_tzd--i"
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XdBbxonmd8-a",
   "metadata": {
    "id": "XdBbxonmd8-a"
   },
   "outputs": [],
   "source": [
    "# Still can see lots of columns with values zero that is also the non essential columns.\n",
    "# Lets drop columns with all the values zero by keeping threshold 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RPzVjIE5cQJr",
   "metadata": {
    "id": "RPzVjIE5cQJr"
   },
   "outputs": [],
   "source": [
    "# Define a threshold for the maximum allowable zero values\n",
    "max_zero_threshold = 1  # This threshold is set to 1, meaning all zeros\n",
    "\n",
    "# Calculate the sum of values equal to zero in each column\n",
    "columns_with_zero_values = (df == 0).sum()\n",
    "\n",
    "# Drop columns with zero values exceeding the threshold\n",
    "columns_to_drop = columns_with_zero_values[columns_with_zero_values > max_zero_threshold].index\n",
    "\n",
    "\n",
    "print(f\"Number of columns with all values 0: {len(columns_to_drop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NofMhDaZebZj",
   "metadata": {
    "id": "NofMhDaZebZj"
   },
   "outputs": [],
   "source": [
    "#Dropping all those 22 columns with all values 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VVUuWlzEcSOK",
   "metadata": {
    "id": "VVUuWlzEcSOK"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cb5e0",
   "metadata": {
    "id": "1c9cb5e0"
   },
   "source": [
    "Total Columns remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5t7M_jBucaft",
   "metadata": {
    "id": "5t7M_jBucaft"
   },
   "outputs": [],
   "source": [
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h35RHtSGcdlr",
   "metadata": {
    "id": "h35RHtSGcdlr"
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae978459",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "ae978459",
    "outputId": "66ec97e5-2283-4197-a7bf-1de522a05163",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c5d33",
   "metadata": {
    "id": "660c5d33"
   },
   "outputs": [],
   "source": [
    "# Now checking one by one and listing out the non essential collumns first then will be removing the data with null values later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ad045",
   "metadata": {
    "id": "807ad045",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa98a31",
   "metadata": {
    "id": "5fa98a31"
   },
   "outputs": [],
   "source": [
    "# id is not essential\n",
    "# member_id is not usually essential. But, lets keep this unique id just in case in future if we want to use other columns which is not in current dataframe, then using id we can map the original_df to the df\n",
    "# loan_amt, funded_amnt, term, int_rate, installment, 'grade', 'sub_grade' are essential columns\n",
    "# emp_title, emp_length is not that essential\n",
    "# home_ownership maybe essential\n",
    "# anual_inc is essential for analysis\n",
    "# verification_status maybe essential\n",
    "# issue_d not essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d51873",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "f6d51873",
    "outputId": "ec42aed7-fbe9-4aba-d4ec-640ba1031017"
   },
   "outputs": [],
   "source": [
    "df['loan_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427faa5d",
   "metadata": {
    "id": "427faa5d"
   },
   "outputs": [],
   "source": [
    "# loan_status is essential column but the rows with 'Current' is not essential Will do row part later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c6e24",
   "metadata": {
    "id": "d63c6e24",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['pymnt_plan'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68da3db",
   "metadata": {
    "id": "e68da3db"
   },
   "outputs": [],
   "source": [
    "# pymnt_plan not essential since all value are 'n'\n",
    "# url, desc not essential for analysis\n",
    "# purpose maybe essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8d225",
   "metadata": {
    "id": "23c8d225",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['title'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886203e",
   "metadata": {
    "id": "1886203e"
   },
   "outputs": [],
   "source": [
    "# title consists of long string and will be difficult for analysis since purpose explains about title, tittle is also non essential\n",
    "# zip_code, addr_state not essential for now, if it we think can be essential later can be added later\n",
    "# earliest_cr_line not essential for now\n",
    "# open_acc, revol_util, total_acc, initial_list_status,'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d' can be essential, if not can be removed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11d68e",
   "metadata": {
    "id": "9d11d68e"
   },
   "outputs": [],
   "source": [
    "df['policy_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784bc09",
   "metadata": {
    "id": "c784bc09"
   },
   "outputs": [],
   "source": [
    "# policy_code is not essential since all the values are '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec624b",
   "metadata": {
    "id": "89ec624b"
   },
   "outputs": [],
   "source": [
    "df['application_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9086116",
   "metadata": {
    "id": "c9086116"
   },
   "outputs": [],
   "source": [
    "# application_type not essential since all the the values are 'INDIVIDUAL' so can be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a253ec5",
   "metadata": {
    "id": "2a253ec5"
   },
   "outputs": [],
   "source": [
    "# Final list of columns that are non essential are:\n",
    "# id, emp_title, emp_length, issue_d, pymnt_plan, url, desc, title, zip_code, addr_state, earliest_cr_line, policy_code, application_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SOhjQltrfGoq",
   "metadata": {
    "id": "SOhjQltrfGoq"
   },
   "outputs": [],
   "source": [
    "df_with_dropped_null_and_zero = df.copy() #Run only once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26CxshiJjF8S",
   "metadata": {
    "id": "26CxshiJjF8S"
   },
   "source": [
    "df = df_with_dropped_null_and_zero.copy #Run only if you need to go back to the last backup dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56ad77",
   "metadata": {
    "id": "cc56ad77"
   },
   "outputs": [],
   "source": [
    "# Deleting the columns that won't be used in further analysis\n",
    "columns_to_delete = ['id', 'emp_title', 'emp_length', 'issue_d', 'pymnt_plan', 'url',\n",
    "                      'title', 'zip_code', 'addr_state', 'earliest_cr_line',\n",
    "                     'last_credit_pull_d', 'policy_code', 'application_type']\n",
    "\n",
    "# Dropping the specified columns\n",
    "df.drop(columns=columns_to_delete, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CAvRz16xfFkj",
   "metadata": {
    "id": "CAvRz16xfFkj"
   },
   "outputs": [],
   "source": [
    "# Count the total number of null values in each column\n",
    "null_value_counts = df.isnull().sum()\n",
    "\n",
    "# Sum the null value counts to get the total number of null values\n",
    "total_null_values = null_value_counts.sum()\n",
    "\n",
    "print(\"Total number of null values:\", total_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09564bc1",
   "metadata": {
    "id": "09564bc1"
   },
   "outputs": [],
   "source": [
    "# Filter for columns with null values\n",
    "columns_with_null_values = null_value_counts[null_value_counts > 0]\n",
    "\n",
    "# Print the column names with null values\n",
    "print(\"Columns with null values:\")\n",
    "print(columns_with_null_values.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fxr-5-HiCkE",
   "metadata": {
    "id": "9fxr-5-HiCkE"
   },
   "outputs": [],
   "source": [
    "# last_pymnt_d can be excluded but revol_util is one of the most important factor to do analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7LS_8PXyiSCD",
   "metadata": {
    "id": "7LS_8PXyiSCD"
   },
   "outputs": [],
   "source": [
    "# Dropping column last_pymnt_d\n",
    "df.drop(\"last_pymnt_d\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WFIZ90o1jl04",
   "metadata": {
    "id": "WFIZ90o1jl04"
   },
   "outputs": [],
   "source": [
    "# Count the total number of null values in each column\n",
    "null_value_counts = df.isnull().sum()\n",
    "\n",
    "# Sum the null value counts to get the total number of null values\n",
    "total_null_values = null_value_counts.sum()\n",
    "\n",
    "print(\"Total number of null values:\", total_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mNFmGOLOj3k0",
   "metadata": {
    "id": "mNFmGOLOj3k0"
   },
   "outputs": [],
   "source": [
    "# Filter for columns with null values\n",
    "columns_with_null_values = null_value_counts[null_value_counts > 0]\n",
    "\n",
    "# Print the column names with null values\n",
    "print(\"Columns with null values:\")\n",
    "print(columns_with_null_values.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DpaY1vtpj41c",
   "metadata": {
    "id": "DpaY1vtpj41c"
   },
   "outputs": [],
   "source": [
    "# The only column left with null value is revol_util. But, revol_util is a percentage of credit balance usage\n",
    "# Can't normalize all null values to zero nor mean.\n",
    "# So lets drop all the rows of revol_util with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wxo3lxamkbe0",
   "metadata": {
    "id": "wxo3lxamkbe0"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X4CtlV5Vkg7W",
   "metadata": {
    "id": "X4CtlV5Vkg7W"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['revol_util'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6-u9wQNqcrvl",
   "metadata": {
    "id": "6-u9wQNqcrvl"
   },
   "outputs": [],
   "source": [
    "# Remove the '%' symbol and convert to float\n",
    "df['revol_util'] = df['revol_util'].str.rstrip('%').astype(float)\n",
    "\n",
    "# Now, the column should contain numerical values\n",
    "df['revol_util'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Sx3vnRNkitd",
   "metadata": {
    "id": "2Sx3vnRNkitd"
   },
   "outputs": [],
   "source": [
    "df_with_dropped_null_and_zero.shape[0] - df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EdHteWo9ktkD",
   "metadata": {
    "id": "EdHteWo9ktkD"
   },
   "outputs": [],
   "source": [
    "# Count the total number of null values in each column\n",
    "null_value_counts = df.isnull().sum()\n",
    "\n",
    "# Sum the null value counts to get the total number of null values\n",
    "total_null_values = null_value_counts.sum()\n",
    "\n",
    "print(\"Total number of null values:\", total_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8NYtmikxNj",
   "metadata": {
    "id": "1a8NYtmikxNj"
   },
   "outputs": [],
   "source": [
    "#Finally we were successfully able to drop all the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef0126",
   "metadata": {
    "id": "0cef0126"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934990db",
   "metadata": {
    "id": "934990db"
   },
   "source": [
    "#### Cleaning Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c7a53",
   "metadata": {
    "id": "197c7a53",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2f037",
   "metadata": {
    "id": "30c2f037"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240567a",
   "metadata": {
    "id": "9240567a"
   },
   "outputs": [],
   "source": [
    "df['loan_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd115fc6",
   "metadata": {
    "id": "bd115fc6"
   },
   "outputs": [],
   "source": [
    "# We don't need 'Current' because they are neither defaulter nor fully paid customer and they won't be used for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145f02e",
   "metadata": {
    "id": "2145f02e"
   },
   "outputs": [],
   "source": [
    "# Keeping only 'Fully Paid' and 'Charged Off' loan_status\n",
    "df = df[df['loan_status'] != 'Current']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5644a32",
   "metadata": {
    "id": "c5644a32"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kQ5qPWDwli67",
   "metadata": {
    "id": "kQ5qPWDwli67"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a9ded",
   "metadata": {
    "id": "6a1a9ded"
   },
   "outputs": [],
   "source": [
    "data_types = df.dtypes\n",
    "print(data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cd1cc",
   "metadata": {
    "id": "715cd1cc"
   },
   "outputs": [],
   "source": [
    "df['term'] = df['term'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155e882",
   "metadata": {
    "id": "5155e882"
   },
   "outputs": [],
   "source": [
    "df['int_rate'] = df['int_rate'].str.rstrip('%').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475e031",
   "metadata": {
    "id": "7475e031"
   },
   "outputs": [],
   "source": [
    "df['grade'] = df['grade'].astype('category')\n",
    "df['sub_grade'] = df['sub_grade'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a307aa0",
   "metadata": {
    "id": "4a307aa0"
   },
   "outputs": [],
   "source": [
    "df['home_ownership'] = df['home_ownership'].astype('category')\n",
    "df['verification_status'] = df['verification_status'].astype('category')\n",
    "df['loan_status'] = df['loan_status'].astype('category')\n",
    "df['purpose'] = df['purpose'].astype('category')\n",
    "df['initial_list_status'] = df['initial_list_status'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cce30",
   "metadata": {
    "id": "9e5cce30"
   },
   "outputs": [],
   "source": [
    "data_types = df.dtypes\n",
    "print(data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb65a3a",
   "metadata": {
    "id": "8fb65a3a"
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ABll8Ff_mQvM",
   "metadata": {
    "id": "ABll8Ff_mQvM"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf1e6cc",
   "metadata": {
    "id": "8cf1e6cc"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Define a list of columns to check for outliers\n",
    "columns_to_check = ['loan_amnt', 'funded_amnt', 'installment', 'annual_inc', 'open_acc', 'revol_util', 'total_acc']\n",
    "\n",
    "# Initialize an empty DataFrame to store Z-scores\n",
    "z_scores = pd.DataFrame()\n",
    "\n",
    "# Calculate Z-scores for each column\n",
    "for col in columns_to_check:\n",
    "    z_scores[col] = np.abs(stats.zscore(df[col]))\n",
    "\n",
    "# Set a threshold for Z-scores (e.g., 3 for significant outliers)\n",
    "threshold = 3\n",
    "\n",
    "# Identify rows with Z-scores beyond the threshold\n",
    "outlier_rows = z_scores[(z_scores > threshold).any(axis=1)]\n",
    "\n",
    "# Print the rows with potential outliers\n",
    "print(outlier_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030ec60",
   "metadata": {
    "id": "f030ec60"
   },
   "outputs": [],
   "source": [
    "# Now lets go through all the columns\n",
    "# loan_amnt and funded_amnt cant have outliers coz there can be people who can take very less loan and there can be people who can take much more loan\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059afb48",
   "metadata": {
    "id": "059afb48"
   },
   "outputs": [],
   "source": [
    "Q1 = df['installment'].quantile(0.25)\n",
    "Q3 = df['installment'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1171bfcd",
   "metadata": {
    "id": "1171bfcd"
   },
   "outputs": [],
   "source": [
    "threshold_multiplier = 1.5\n",
    "lower_threshold = Q1 - threshold_multiplier * IQR\n",
    "upper_threshold = Q3 + threshold_multiplier * IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde2a7f",
   "metadata": {
    "id": "4dde2a7f"
   },
   "outputs": [],
   "source": [
    "df = df[(df['installment'] >= lower_threshold) & (df['installment'] <= upper_threshold)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc45afc",
   "metadata": {
    "id": "afc45afc"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118dfe17",
   "metadata": {
    "id": "118dfe17"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select numeric columns from the DataFrame\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Create subplots for each numeric column\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "sns.boxplot(data=numeric_columns, orient=\"h\", palette=\"Set2\")\n",
    "plt.title(\"Box Plot of Numeric Columns\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d978712",
   "metadata": {
    "id": "0d978712"
   },
   "outputs": [],
   "source": [
    "# Calculate the IQR for annual_inc\n",
    "Q1 = df['annual_inc'].quantile(0.25)\n",
    "Q3 = df['annual_inc'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['annual_inc'] < Q1 - threshold) | (df['annual_inc'] > Q3 + threshold)]\n",
    "\n",
    "# Remove outliers\n",
    "df_clean = df[~df.index.isin(outliers.index)]\n",
    "\n",
    "# Check the shape of the cleaned DataFrame\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"Cleaned DataFrame shape:\", df_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a8701",
   "metadata": {
    "id": "5b2a8701"
   },
   "outputs": [],
   "source": [
    "df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420e3c0",
   "metadata": {
    "id": "e420e3c0"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f72627",
   "metadata": {
    "id": "41f72627"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select numeric columns from the DataFrame\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Create subplots for each numeric column\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "sns.boxplot(data=numeric_columns, orient=\"h\", palette=\"Set2\")\n",
    "plt.title(\"Box Plot of Numeric Columns\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c2617",
   "metadata": {
    "id": "301c2617"
   },
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis (EDA): Explore the cleaned dataset to gain insights and understand the data better.\n",
    "# Summary statistics\n",
    "summary_stats = df.describe()\n",
    "\n",
    "# Value counts for categorical columns\n",
    "categorical_columns = ['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status', 'loan_status', 'purpose', 'initial_list_status']\n",
    "value_counts = {col: df[col].value_counts() for col in categorical_columns}\n",
    "\n",
    "# Correlation matrix (if applicable for numeric columns)\n",
    "correlation_matrix = df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608c1ac",
   "metadata": {
    "id": "b608c1ac"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the distribution of loan amounts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['loan_amnt'], bins=30, kde=True)\n",
    "plt.title('Distribution of Loan Amount')\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Visualize loan status counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df, x='loan_status', palette='Set2')\n",
    "plt.title('Loan Status Counts')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Box plot of loan amount by loan status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='loan_status', y='loan_amnt', palette='Set2')\n",
    "plt.title('Loan Amount by Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Loan Amount')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of annual income and loan amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='annual_inc', y='loan_amnt', hue='loan_status', palette='Set2')\n",
    "plt.title('Scatter Plot of Annual Income vs Loan Amount')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Loan Amount')\n",
    "plt.show()\n",
    "\n",
    "# Plotting count of grades\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df, x='grade', palette='Set2')\n",
    "plt.title('Grade Counts')\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for numeric variables\n",
    "corr_matrix = df[['loan_amnt', 'annual_inc', 'int_rate', 'installment', 'open_acc', 'total_acc']].corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Visualize 'revol_util' using a histogram\n",
    "df['revol_util'] = df['revol_util'].str.rstrip('%').astype(float)  # Convert 'revol_util' to float\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['revol_util'], bins=30, kde=True)\n",
    "plt.title('Distribution of Revolving Line Utilization Rate')\n",
    "plt.xlabel('Revolving Line Utilization Rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate analysis: 'revol_util' vs 'loan_status'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='loan_status', y='revol_util', palette='Set2')\n",
    "plt.title('Revolving Line Utilization Rate by Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Revolving Line Utilization Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot for 'total_acc'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['total_acc'], bins=30, kde=True)\n",
    "plt.title('Distribution of Total Accounts')\n",
    "plt.xlabel('Total Accounts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Segmented univariate analysis: 'total_acc' by 'loan_status'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='loan_status', y='total_acc', palette='Set2')\n",
    "plt.title('Total Accounts by Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Total Accounts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x4P1wxE6qYIA",
   "metadata": {
    "id": "x4P1wxE6qYIA"
   },
   "outputs": [],
   "source": [
    "df['loan_status'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7nfkjK0qYF1",
   "metadata": {
    "id": "q7nfkjK0qYF1"
   },
   "outputs": [],
   "source": [
    "df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mF_t6vZTqYCu",
   "metadata": {
    "id": "mF_t6vZTqYCu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L481GpzbqYAV",
   "metadata": {
    "id": "L481GpzbqYAV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QO5IqtqkqX9d",
   "metadata": {
    "id": "QO5IqtqkqX9d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RxDCrK8hqX6-",
   "metadata": {
    "id": "RxDCrK8hqX6-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnmETOIsqX42",
   "metadata": {
    "id": "DnmETOIsqX42"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xMEB-WaCqX1t",
   "metadata": {
    "id": "xMEB-WaCqX1t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t2-rkCG2qXzF",
   "metadata": {
    "id": "t2-rkCG2qXzF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xNNid1COqXxH",
   "metadata": {
    "id": "xNNid1COqXxH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acDB9AXTqXuF",
   "metadata": {
    "id": "acDB9AXTqXuF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m5oQye06qXro",
   "metadata": {
    "id": "m5oQye06qXro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_lDe2F4lqXog",
   "metadata": {
    "id": "_lDe2F4lqXog"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ufxgibk_qXln",
   "metadata": {
    "id": "Ufxgibk_qXln"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LIKT96V8qXjF",
   "metadata": {
    "id": "LIKT96V8qXjF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ab8c88",
   "metadata": {
    "id": "c6ab8c88"
   },
   "source": [
    "# ..................................................................................................................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rkbW8cD1p_i8",
   "metadata": {
    "id": "rkbW8cD1p_i8"
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GRSvkzdDp_cb",
   "metadata": {
    "id": "GRSvkzdDp_cb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5721849",
   "metadata": {
    "id": "f5721849",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of non-essential columns to drop\n",
    "non_essential_columns = ['id', 'pymnt_plan','url', 'desc', 'policy_code',\n",
    "                         'application_type', 'annual_inc_joint','dti_joint','verification_status_joint', 'next_pymnt_d', 'tax_liens', 'initial_list_status']\n",
    "\n",
    "# Drop non-essential columns\n",
    "loan_data = loan_data.drop(columns=non_essential_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47638d",
   "metadata": {
    "id": "4e47638d"
   },
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = loan_data.isnull().sum()\n",
    "\n",
    "# Display the columns with missing values and the count of missing values\n",
    "print(\"Columns with Missing Values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check for outliers or inconsistencies as needed\n",
    "# You can use descriptive statistics (e.g., describe()) to identify outliers.\n",
    "\n",
    "# Interpret the meanings of variables by referring to the Data Dictionary\n",
    "# Ensure that you understand what each variable represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c83f8d",
   "metadata": {
    "id": "e2c83f8d"
   },
   "outputs": [],
   "source": [
    "loan_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab4b64",
   "metadata": {
    "id": "32ab4b64"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate summary statistics for loan_amnt and annual_inc\n",
    "loan_data[['loan_amnt', 'annual_inc']].describe()\n",
    "\n",
    "# Create box plots for loan_amnt and annual_inc\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.boxplot(loan_data['loan_amnt'], vert=False)\n",
    "plt.title('Box Plot for loan_amnt')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.boxplot(loan_data['annual_inc'], vert=False)\n",
    "plt.title('Box Plot for annual_inc')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c248f",
   "metadata": {
    "id": "539c248f"
   },
   "outputs": [],
   "source": [
    "# Check unique values in the home_ownership and grade columns\n",
    "print(\"Unique values in home_ownership:\")\n",
    "print(loan_data['home_ownership'].unique())\n",
    "\n",
    "print(\"\\nUnique values in grade:\")\n",
    "print(loan_data['grade'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248c60d",
   "metadata": {
    "id": "b248c60d"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09545daa",
   "metadata": {
    "id": "09545daa"
   },
   "outputs": [],
   "source": [
    "# Identify columns with missing values\n",
    "missing_values = original_df.isnull().sum()\n",
    "\n",
    "# Loop through columns and fill missing values\n",
    "for column in original_df.columns:\n",
    "    if missing_values[column] > 0:\n",
    "        if original_df[column].dtype == 'object':\n",
    "            # For categorical columns, fill with the mode\n",
    "            original_df[column].fillna(original_df[column].mode()[0], inplace=True)\n",
    "        else:\n",
    "            # For numeric columns, fill with the mean\n",
    "            original_df[column].fillna(original_df[column].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e53a4",
   "metadata": {
    "id": "e77e53a4"
   },
   "outputs": [],
   "source": [
    "# Impute missing values in numerical columns with the mean\n",
    "numerical_columns = loan_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "loan_data[numerical_columns] = loan_data[numerical_columns].fillna(loan_data[numerical_columns].mean())\n",
    "\n",
    "# Impute missing values in categorical columns with the mode\n",
    "categorical_columns = loan_data.select_dtypes(include=['object']).columns\n",
    "loan_data[categorical_columns] = loan_data[categorical_columns].fillna(loan_data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Check if there are any remaining missing values\n",
    "remaining_missing = loan_data.isnull().sum().sum()\n",
    "print(\"Remaining Missing Values:\", remaining_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5464714",
   "metadata": {
    "id": "c5464714"
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in each column\n",
    "missing_percentage = (loan_data.isnull().sum() / len(loan_data)) * 100\n",
    "\n",
    "# Set a threshold for missing values (e.g., 80%)\n",
    "threshold = 80\n",
    "\n",
    "# Identify columns with missing values exceeding the threshold\n",
    "high_missing_columns = missing_percentage[missing_percentage > threshold].index.tolist()\n",
    "\n",
    "# Decide whether to drop or impute these columns\n",
    "# For example, you can drop them as follows:\n",
    "loan_data = loan_data.drop(columns=high_missing_columns)\n",
    "\n",
    "# Impute missing values in remaining columns with sentinel values\n",
    "# For example, you can impute missing values in numerical columns with -1\n",
    "numerical_columns = loan_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "loan_data[numerical_columns] = loan_data[numerical_columns].fillna(-1)\n",
    "\n",
    "# Impute missing values in categorical columns with 'Unknown'\n",
    "categorical_columns = loan_data.select_dtypes(include=['object']).columns\n",
    "loan_data[categorical_columns] = loan_data[categorical_columns].fillna('Unknown')\n",
    "\n",
    "# Check if there are any remaining missing values\n",
    "remaining_missing = loan_data.isnull().sum().sum()\n",
    "print(\"Remaining Missing Values:\", remaining_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84aa4b3",
   "metadata": {
    "id": "c84aa4b3"
   },
   "outputs": [],
   "source": [
    "# Identify date columns\n",
    "date_columns = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d']\n",
    "\n",
    "# Convert date columns to datetime\n",
    "for col in date_columns:\n",
    "    loan_data[col] = pd.to_datetime(loan_data[col], format='%b-%y', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0573c",
   "metadata": {
    "id": "cee0573c"
   },
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format (if applicable)\n",
    "loan_data['issue_d'] = pd.to_datetime(loan_data['issue_d'])\n",
    "\n",
    "# Handle string columns as needed (e.g., removing special characters, converting to lowercase, etc.)\n",
    "# For example, you can clean the 'term' column to extract the numeric part.\n",
    "loan_data['term'] = loan_data['term'].str.extract('(\\d+)').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9fb48",
   "metadata": {
    "id": "b4b9fb48"
   },
   "outputs": [],
   "source": [
    "# Example: Create a new variable 'loan_to_income_ratio'\n",
    "loan_data['loan_to_income_ratio'] = loan_data['loan_amnt'] / loan_data['annual_inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e28e26",
   "metadata": {
    "id": "79e28e26"
   },
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a new CSV file\n",
    "loan_data.to_csv(\"cleaned_loan_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e20483",
   "metadata": {
    "id": "d6e20483"
   },
   "outputs": [],
   "source": [
    "loan_data = pd.read_csv('cleaned_loan_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe7830",
   "metadata": {
    "id": "0dbe7830"
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_stats = loan_data.describe()\n",
    "\n",
    "# Histogram of loan amounts\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(loan_data['loan_amnt'], bins=20, edgecolor='k')\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Loan Amounts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3212861",
   "metadata": {
    "id": "b3212861"
   },
   "outputs": [],
   "source": [
    "# Example: Box plot of loan amounts by loan status\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='loan_status', y='loan_amnt', data=loan_data)\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Loan Amount')\n",
    "plt.title('Loan Amounts by Loan Status')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b471e33",
   "metadata": {
    "id": "3b471e33"
   },
   "outputs": [],
   "source": [
    "# Example: Correlation heatmap of numerical variables\n",
    "correlation_matrix = loan_data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c38086",
   "metadata": {
    "id": "12c38086"
   },
   "outputs": [],
   "source": [
    "# Example: Analyzing the loan-to-income ratio\n",
    "plt.hist(loan_data['loan_to_income_ratio'], bins=20, edgecolor='k')\n",
    "plt.xlabel('Loan-to-Income Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Loan-to-Income Ratio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1b7ac",
   "metadata": {
    "id": "ddd1b7ac"
   },
   "outputs": [],
   "source": [
    "loan_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb657b1",
   "metadata": {
    "id": "3cb657b1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543414e",
   "metadata": {
    "id": "2543414e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bba85",
   "metadata": {
    "id": "6a7bba85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1cbd4",
   "metadata": {
    "id": "21b1cbd4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68783c51",
   "metadata": {
    "id": "68783c51"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df7e91",
   "metadata": {
    "id": "92df7e91"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f801097",
   "metadata": {
    "id": "7f801097"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd99608",
   "metadata": {
    "id": "cdd99608"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23163d",
   "metadata": {
    "id": "9f23163d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b984fa",
   "metadata": {
    "id": "e0b984fa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ANNzunYWIsjh",
    "y8Xy30lDN-P9",
    "l_7LDA_dI_6R",
    "TxJN4WezWXiG",
    "MnfAKeCPJW44",
    "2Hrr7m804OYg",
    "DZmDPbJiexOf",
    "Wa-mlH6iw-4H",
    "jJETaHE27wnL"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
